Here we are going to implement the GPT-2 and load OpenAI`s weights.

This implamentation can be found with more details in the book [Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167?sr=8-1&language=en_US&ref_=as_li_ss_tl) by Sebastian Raschka  

The author show how to build the attention blocks, the transformers and also how to train the GPT-2 model
however, instead of training the model, which requires a lot of data and compute power. here we will only define the GPT and load OpenAI`s weights on to it.

you can check the entire process of building the GPT-2 with more details in the following links:

[chapter 3](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb)
[chapter 4](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch04/01_main-chapter-code/ch04.ipynb)
[chapter 5](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb)

there is also a greate youtube video that explains how to do the GPT from scratch from andrej karpathy

[video](https://www.youtube.com/watch?v=kCc8FmEb1nY)